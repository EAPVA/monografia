\chapter{CUDA: Arquitetura e Programação}\label{cap:recursos}

\sigla{CUDA}{Compute Unified Device Architecture}(\emph{Compute Unified Device
                                                  Architecture}) é um
\emph{framework} de programação paralela desenvolvido pela
NVIDIA\footnote{https://developer.nvidia.com}, envolvendo várias
ferramentas necessárias para tal, como
arquitetura de \emph{hardware}, \sigla{API}{Application Programming
  Interface}(\emph{Application Programming Interface} -
  Interface de programação de aplicações), bibliotecas, compilador e
  \sigla{IDE}{Integrated Development Environment}(\emph{Integrated Development
                                                  Environment} - Ambiente de
                                                  desenvolvimento integrado).
  Ele permite realizar programação de propósito geral para 
  GPU (\emph{Graphical Processing Unit} - Unidade de processamento gráfico)
  utilizando linguagens de alto nível, como C/C++, Fortran e Python
  \cite{nvidialanguage}.

Como uma GPU não é ideal para várias aplicações de propósito geral, esta é
usada junto a uma CPU (\emph{Central Processing Unit} - unidade central de 
                       processamento), que irá
chamar funções para serem executadas na GPU. Essa CPU é chamada de \emph{host},
       enquanto a GPU é chamada de \emph{device}. O fluxo padrão de execução é
       representado na Figura \ref{fig:GPUhost}.

\begin{figure}[h!]
  \centering
  \caption{Fluxo de execução padrão usando CUDA.}
  \includegraphics[keepaspectratio=true,width=0.9\textwidth]
    {images/gpuhost.png}
      \fonte{Wikipedia, disponível em:
        <https://commons.wikimedia.org/wiki/File:CUDA\_processing\_flow\_(En).PNG>
      Acesso em jun. 2015}
  \label{fig:GPUhost}
\end{figure}

\section{Arquitetura de GPUs}

A arquitetura de uma GPU difere de uma CPU
  principalmente por
  ser voltada a executar uma instrução em um conjunto de dados, ao invés de um
  único dado. Para isso, as GPUs fazem uma troca entre estruturas avançadas de
  controle de execução de código por estruturas de processamento 
  lógico-aritimético, como demonstra a Figura \ref{fig:GPUarch}. 
  Um conjunto de núcleos
  que compartilha a mesma estrutura de controle é denominado \emph{Stream
    Multiprocessor} (\sigla{SM}{Stream Multiprocessor}).
    \footnote{Na arquitetura Kepler é
    chamado de \emph{Next Generation Stream Multiprocessor} (\sigla{SMX}{Next
                                                       Generation Stream
                                                       Multiprocessor}) e na
      Maxwell de \emph{Maxwell Stream Multiprocessor} (\sigla{SMM}{Maxwell
                                                       Stream
                                                       Multiprocessor}).}
  
\begin{figure}[h!]
  \centering
  \caption{Diferença na arquitetura de uma CPU, à esquerda e uma GPU, à
    direita. As estruturas de controle de execução de código são compartilhadas
      entre vários núcleos.}
  \includegraphics[keepaspectratio=true,width=0.9\textwidth]
    {images/gpuarch.png}
        \fonte{\emph{Personal Blog of} Mohamed F. Ahmed, disponível em:
          <https://mohamedfahmed.wordpress.com/2010/05/03/cuda-computer-unified-device-architecture/>
      Acesso em jun. 2015}
  \label{fig:GPUarch}
\end{figure}

Cada SM possui um banco de registradores, que é particionado entre todos os
  seus núcleos, e três regiões de memória que são compartilhadas: memória
  compartilhada, cache de textura e cache de constantes. A Figura \ref{fig:GPUmem}
demonstra essas regiões de memória.

\begin{figure}[h!]
  \centering
  \caption{Representação da arquitetura de memória de uma GPU.}
  \includegraphics[keepaspectratio=true,width=0.9\textwidth]
    {images/gpumem.png}
  \fonte{NVIDIA's \emph{Parallel Thread Execution} \sigla{ISA}{Instruction Set
    Architecture} (\emph{Instruction Set Architecture} - arquitetura de 
                   conjunto de instruções) \emph{Documentation}, disponível em:
    <http://docs.nvidia.com/cuda/parallel-thread-execution/\#set-of-simt-multiprocessors-with-on-chip-shared-memory>
      Acesso em jun. 2015}
  \label{fig:GPUmem}
\end{figure}

Cada tipo de memória tem vantagens e desvantagens.
  O acesso a memória compartilhada é mais rápido do que
  o acesso a outros tipos de memória, as caches de constantes e de texturas são
  somente-leitura(para o processador da GPU) e a última tem estruturas
  especiais para fazer interpolação dos dados contidos nesta.

GPUs \emph{dedicadas} são fisicamente separadas da CPU \emph{host} e, portanto,
     possuem memória global (\emph{Device Memory} na Figura \ref{fig:GPUmem}) 
       separada da memória RAM do \emph{host}. GPUs \emph{integradas} possuem
       acesso direto à memória RAM do \emph{host}, e não é necessário copiar
       memória entre a CPU e a GPU, acelerando a execução.

Cada SM tem, também, uma ou mais \emph{warps}. 
Dentro de um \emph{warp} não é possível executar duas instruções diferentes
  ao mesmo tempo. Caso isso ocorra (por causa de execução condicional no
                                    código), o multiprocessador serializa as 
  instruções. Em todas as GPUs da NVIDIA capazes de realizar processamento de
  propósito geral, o tamanho do \emph{warp} é 32 núcleos \cite{nvidiawarp}.

\subsection{Acesso otimizado de memória}

Para otimizar o acesso a memória é necessário ter em mente algumas restrições.
Cada tipo de memória possui uma maneira específica de otimizar o acesso. A
seguir são apresentadas as restrições necessárias para o acesso à memória
global e memória compartilhada, usadas nesse trabalho.

\subsubsection{Memória Global}

A memória global é lida para o dispositivo em transações 
de 32, 64 ou 128 \emph{bytes} alinhadas, ou seja, o primeiro endereço deve ser
múltiplo do número de \emph{bytes} lidos. Caso as \emph{threads}
consecutivo dentro de um \emph{warp} 
acessem endereços consecutivos da memória, essa
leitura é \emph{coalescida} em uma única transação, desde que o tamanho do
elemento acessado por cada \emph{thread} seja 1, 2, 4, 8 ou 16 \emph{bytes}
\ref{nvidiawarp}.

Acesso não-alinhado de memória resulta em transações extras, lendo mais grupos
de 32, 64 ou 158 \emph{bytes}, causando uma perda de desempenho. Acesso de
memória com endereços distantes uma da outra, dentro de um \emph{warp}, causa
grandes perdas de desempenho no acesso à memória.

\subsubsection{Memória Compartilhada}

A memória compartilhada é particionada em 32 bancos de memória intercalados, de
maneira que palavras consecutivas de 32 ou 64 \emph{bits} (dependo da
configuração do modo da memória compartilhada) são mapeados para bancos
consecutivos. Em outras palavras, a primeira palavra está no primeiro banco, a
segunda no segundo, e assim por diante até o trigésimo segundo banco, depois do
qual o processo recomeça.
Quando duas \emph{threads} dentro de um \emph{warp} acessam duas palavras
diferentes dentro do mesmo banco, ocorre um \emph{conflito de banco}. Caso um
conflito de banco ocorra, o acesso a memória é serializado entre as
\emph{threads} correspondentes. Duas \emph{threads} acessando exatamente o
mesmo endereço de memória não causa um conflito, resultando em um
\emph{broadcast} no caso de uma leitura e apenas uma das \emph{threads}
realizando a gravação, no caso de uma gravação de memória (qual \emph{thread},
exatamente, é indeterminado).

\section{Programação em CUDA}

O modelo de programação usando a API de programação CUDA é baseado na
  arquitetura física da GPU, com sua separação em \emph{Stream Multiprocessors}
e núcleos. Uma função da GPU é executada várias vezes, dependendo dos
  parâmetros de configuração usados.

  As funções a serem executadas na GPU são chamadas de \emph{kernels}.
  O \emph{host} configura e inicia a execução assíncrona dos \emph{kernels},
    usando dois parâmetros para escolher o número de vezes que o \emph{kernel}
será executado: número de blocos e \emph{threads} por bloco. Cada \emph{thread}
é executada em um único núcleo e cada bloco é executado em um único SM. A
  divisão dos blocos em \emph{warps} e o escalonamento destes são feitos
  automaticamente pelo SM, sem garantia da ordem de execução. Caso ocorra
  serialização dentro de um \emph{warp}, também não há garantia da ordem de
  execução. A configuração do número de blocos para a execução de um
  \emph{kernel} é chamada de \emph{grid}.

  De maneira a facilitar a programação para aplicações onde os dados são
  organizados logicamente de maneira 2D (como, por exemplo, processamento de
                                         imagens) ou 3D (como, por exemplo,
                                                         simulações físicas),
  as \emph{threads} e blocos podem ser configurados com uma estrutura especial
    representando um escalar inteiro de três dimensões 
    (duas dimensões são alcançadas fazendo com que a terceira tenha tamanho 1).
    A Figura \ref{fig:GPUgrid} representa a organização em blocos de 
    \emph{threads} em duas dimensões.

\begin{figure}[h!]
  \centering
  \caption{\emph{Grid} de blocos de \emph{threads} de duas dimensões.}
  \includegraphics[keepaspectratio=true,width=0.9\textwidth]
    {images/gpugrid.png}
    \fonte{\emph{CUDA C Progamming Guide}, disponível em:
      <http://docs.nvidia.com/cuda/cuda-c-programming-guide/\#thread-hierarchy>
      Acesso em jun. 2015}
  \label{fig:GPUgrid}
\end{figure}

Cada \emph{thread} em um bloco tem um \emph{id} único e cada bloco em um
  \emph{grid} tem, também, um \emph{id} único. Ambos os \emph{ids} são
  representados usando um escalar de três dimensões. Cada \emph{thread} tem
  acesso, em tempo de execução, a quatro constantes escalares tridimensionais
  representando: \emph{id} da \emph{thread} atual dentro do bloco, \emph{id} do
  bloco a qual a \emph{thread} pertence, dimensões do tamanho do bloco e
  dimensões do tamanho do \emph{grid}. Essas constantes podem ser usadas para
  localizar em quais posições na memória a \emph{thread} deve operar.

