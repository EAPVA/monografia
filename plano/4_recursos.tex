\chapter{CUDA: Arquitetura e Programação}\label{cap:recursos}

CUDA, \emph{Compute Unified Device Architecture}, é uma arquitetura de hardware 
e software, desenvolvida pela NVidia, que permite
programar uma GPU para executar código de aplicações gerais escrita em linguagens
de programação de alto nível.%\ref{http://www.nvidia.com.br/object/cuda_home_new_br.html}

CUDA nasceu como uma solução para os problemas do modelo GPGPU, onde a API gráfica
era utilizada para computação de propósito geral. No modelo GPGPU, o programador 
deveria ter um grande conhecimento sobre o funcionamento da GPU e da API gráfica.
Os problemas tinham que ser descritos como coordenadas de vértices, texturas e
\emph{shaders}, o que aumentava sua complexidade. Por último, GPUs não tinham
suporte a características básicas de programação, e a falta de suporte a variáveis
de ponto flutuante de dupla precisão restringia os modelos de programação que 
poderiam ser utilizados.
  

\section{GPU}

Uma Graphics Processor Unit (GPU) é uma arquitetura computacional especializada 
para tarefas com alta demanda computacional e altamente paralelizáveis, devido ao
fato que GPUs possuem centenas de processadores, otimizados para executarem 
tarefas de forma paralela.

Na década de 70, chips dedicados para acelerar o desenho de gráficos 
eram utilizados em placas-mãe de jogos de fliperama. Desde então, esses chips
evoluíram drasticamente suas funções e capacidade de processamento devido à demanda
por gráficos 3D em tempo real, eventualmente tornando-se as GPUs que conheçemos atualmente.
Em 1986, a Texas Instruments lançou o primeiro microprocessador com uma GPU 
capaz de executar código para aplicações gerais. Porém, apenas em 2001, se tornou mais prático 
utilizar GPUs para aplicações de uso geral.  

Atualmente, GPUs podem ser encontrada em computadores pessoais, laptops,
celulares e até mesmo em placas embarcadas. Uma GPU discreta possui uma memória 
RAM própria e uma quantidade maior de núcleos se comparada com GPUs integradas. 
No entanto, as GPUs discretas também são mais caras, sendo comumente utilizadas
para aplicações gráficas e jogos eletrônicos. As GPUs integradas são encontradas
junto a CPU na mesma pastilha e utilizam parte da memória RAM do sistema
e não possuem tantos núcleos quanto GPUs discretas, portanto possuem menor
capacidade computacional e são mais baratas. 

Uma GPU possui centenas de processadores especializados para operações paralelas e
com uma alta densidade de transistores. O núcleo de uma GPU é dedicado à executar
um conjunto de instruções único de maneira mais rápida possível, portanto sua 
arquitetura é muito mais simples se comparada à uma CPU que pode receber centenas
de conjuntos de instruções.

%Figura CPU Style Cores x Slimming Down (GPU Cores)
%Fonte http://haifux.org/lectures/267/Introduction-to-GPUs.pdf

Os núcleos partilham diferentes tipos de espaços de memória e saber qual memória deve ser 
utilizada é pivotal para um bom desempenho. Registradores e memória local são
apenas visíveis para a linha de execução que escreve nessa memória e os dados duram
enquanto essa linha de execução estiver ativa. A memória local é um pouco mais
lenta do que os registradores.

A memória compartilhada é visível por todas as linhas de execução presentes em um
mesmo bloco, permitindo que essas linhas de execução comuniquem-se e troquem dados
entre si. A memória global é visível por todas as linhas de execução da aplicação
e pelo host e os dados são mantidos pela duração da alocação de memória feita
pelo host. 

Outros tipos de memória são a memória de texturas e a memória de constantes. 
A memória de constantes guarda todas as contantes utilizadas pela aplicação,
efetivamente servindo apenas para leitura após o inicío da execução. A memória
de textura guarda informações de texturas e também é utilizada como uma memória
apenas para leitura. Ambas são beneficas para tipos especificos de aplicações.

%Figura Tipos de memória
%Fonte https://www.microway.com/hpc-tech-tips/gpu-memory-types-performance-comparison/


O funcionamento de uma GPU se resume a receber informações geométricas da CPU como
entrada e emitir uma imagem como saída. O processo se iniciar com a interface com a host,
onde a GPU irá receber os comandos da CPU e carregar as informações de geometria
da memória. O resultado desse primeiro passo será um conjunto de vértices com
informações associadas, como texturas, coordenadas, normais, entre outros.

O conjunto de vértices resultante da comunicação com o host está em função do
espaço do objeto, sendo necessário transformá-lo para ser exibido no espaço da tela
do computador. Essa fase é chamada de processamento de vértices, onde todos os 
dados recebidos na etapa anterior são modificados para serem exibidos na tela. 
Nessa etapa não há a criação ou o descarte de vértices, portanto todos os vértices 
recebidos da interface com o host são transformados.

Após definir os locais dos vértices na tela, criam-se os triângulos que irão 
preencher a tela. Antes de preencher a tela, triângulos fora do campo de visão 
são rejeitados, sendo apenas utilizados os triângulos que irão compor os pixels
da tela. Essa fase é chamada de configuração de triângulos. 

Após inserir os triângulos na tela, toda informação de texturas, normais, posição,
é utilizada para definir a cor final do pixel. Aqui são utilizadas técnicas como 
operações matemáticas e mapeamento de texturas para colorir o pixel.Essa etapa é 
chamada de processamento de pixels.

Por último, a informação dos pixels é escrita para um framebuffer e a imagem
final é demonstrada na tela. A última etapa é chamada interface de memória. 

%Figura pipeline

\section{Arquitetura}

%ftp://download.nvidia.com/developer/cuda/seminar/TDCI_Arch.pdf
%http://docs.nvidia.com/cuda/cuda-c-programming-guide/#axzz3dtAz2c5O

A arquitetura CUDA foi desenvolvida pela NVidia com o objetivo de otimizar o uso de
GPUs para aplicações de uso geral, utilizando código escrito em linguagens de 
programação de alto nível. 

Uma GPU consiste de centenas de processadores em paralelo, interligados por interfaces 
de memória muito mais rápidas e complexas. 

Um núcleo CUDA é um núcleo de GPU criado para ser programado para aplicações gerais. 
Esses núcelos seguem o paradigma 
SIMD (Single Instruction, Multiple Data), onde um grupo de processadores executam
a mesma instrução de forma paralela, porém cada processador é encarregado
de um pacote de dados diferente. 

GPUs utilizam a arquitetura SIMD (Single Instruction, Multiple Data), na qual 
a mesma instrução é executada em diferentes dados. 


\section{Programação}

CUDA, uma \emph{framework} desenvolvida pela NVidia, permite que se utilize uma
GPU para processamento de propósito geral, também conhecido como GPGPU
(\emph{General-purpose computing on graphics processing units}). O
objetivo é que aplicações possam utilizar a arquitetura paralela e velocidade
fornecidas por uma GPU para fins além de aplicações gráficas.

A API permite organizar as threads, linhas de execução de programa que desenvolvem
atividades de forma concorrente, em blocos e criar conjuntos de
blocos chamados grids. É comum utilizarmos um número muito maior de blocos nos grids
do que os fisicamente disponíveis na placa, então a API se encarrega de
realizar o escalonamento automático das atividades entre os processadores e
blocos fisicamente existentes. 

Cada thread e bloco possui uma ID própria, sendo possível utilizar essa informação para que cada thread
trabalhe com um conjunto de dados únicos. A API permite também organizar as threads em blocos e grids de maneira 2D ou 3D,
ficando o ID da Thread ou Bloco como uma tupla. Em processamento de imagens, as tarefas são usualmente realizadas em matrizes de duas 
dimensões, portanto utilizar uma organização em 2D facilita a escrita de código, com cada Thread processando um pixel da imagem.

Sendo possível programar e executar centenas de threads de forma única, também
é provável que existam milhares de erros e bugs à serem consertados. Como o
programa principal e cada thread são executados de forma independente, não há
uma falha ou aviso quando algo está errado. É necessário esperar que as threads
terminem suas execuções e sincronizá-las com o programa para que possamos
procurar o erro. A API possui funções que permitem extrair qual o último erro
encontrado na execução e assim podemos eliminar bugs de nossas aplicações.

